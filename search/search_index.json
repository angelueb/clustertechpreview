{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hey there...","text":""},{"location":"#description","title":"Description","text":"<p>This is an ongoing technical overview of our unit's computing cluster. It's intended to start as a simple informal reference, and adapt its extension and depth according to more defined criteria and/or requirements in the future  </p>"},{"location":"baseoss/","title":"Base Operating Systems","text":"<p>For stability, availability and support reasons, all the cluster nodes are running binary compatible Read Hat Linux distributions.</p>"},{"location":"baseoss/#rocky-linux","title":"Rocky Linux","text":"<p>The master node and the all the compute nodes run Rocky Linux 8.x. At the time of writing, the currently used versions are 8.7 and 8.6.</p>"},{"location":"baseoss/#centos","title":"CentOS","text":"<p>The strorage node runs CentOS 7.9</p>"},{"location":"basicaccesperm/","title":"User access and security","text":""},{"location":"basicaccesperm/#vpn","title":"VPN","text":"<p>Connection through the institution's VPN is currenlty mandatory. For external users, please contact the administrators</p>"},{"location":"basicaccesperm/#user-permissions","title":"User permissions","text":"<p>User will not be granted any kind of administrative permissions. For specific permissions not included in the standard sets, users are asked to contact the administrators</p>"},{"location":"basicaccesperm/#network-ports","title":"Network Ports","text":"<p>Similarly, due to security reasosn, only network ports considered needed for regular operations in the cluster are open. For opening additional ports, users are asked to contact the administartors.</p>"},{"location":"basicaccessperm/","title":"User access and security","text":""},{"location":"basicaccessperm/#vpn","title":"VPN","text":"<p>Connection through the institution's VPN is currenlty mandatory. For external users, please contact the administrators</p>"},{"location":"basicaccessperm/#user-permissions","title":"User permissions","text":"<p>User will not be granted any kind of administrative permissions. For specific permissions not included in the standard sets, users are asked to contact the administrators</p>"},{"location":"basicaccessperm/#network-ports","title":"Network Ports","text":"<p>Similarly, due to security reasosn, only network ports considered needed for regular operations in the cluster are open. For opening additional ports, users are asked to contact the administartors.</p>"},{"location":"nodeshard/","title":"Summary of node's roles and hardware","text":""},{"location":"nodeshard/#roles","title":"Roles","text":""},{"location":"nodeshard/#master","title":"Master","text":"<p>In our case, this role includes:</p> <ul> <li>Cluster control and management</li> <li>Provision and orchestration</li> <li>Login</li> <li>Job and resource management</li> <li>Job submition</li> </ul>"},{"location":"nodeshard/#compute","title":"Compute","text":"<p>Nodes with this role actually run the jobs assigned by SLURM. There are specific nodes for CPU and for GPGPU computing.</p>"},{"location":"nodeshard/#storage","title":"Storage","text":"<p>In our case, it provides the shared storage resources that contain user related data (home) and immediate computing jobs data (scratch)</p>"},{"location":"nodeshard/#hardware-summary","title":"Hardware summary","text":"<ul> <li> <p>Node name: avicenna</p> <ul> <li>Cluster role: Master</li> <li>CPU: AMD Epyc<ul> <li>Cores/Threads (total): 16/32</li> </ul> </li> <li>RAM: 128GB</li> <li>Network links<ul> <li>BMC (VHIR)</li> <li>External Acess (VHIR)</li> <li>Internal</li> </ul> </li> </ul> </li> <li> <p>Node Name: c01</p> <ul> <li>Cluster role: Compute<ul> <li>Type: CPU</li> </ul> </li> <li>CPU: AMD Epyc x2<ul> <li>Cores/Threads (total): 32/64</li> </ul> </li> <li>RAM: 512GB</li> <li>Network links<ul> <li>BMC (VHIR)</li> <li>Internal</li> </ul> </li> </ul> </li> <li> <p>Node Name: g01</p> <ul> <li>Cluster role: Compute<ul> <li>Type: GPGPU</li> </ul> </li> <li>CPU: AMD Epyc x2<ul> <li>Cores/Threads (total): 24</li> </ul> </li> <li>GPU: Ampere x 2 (24GB VRAM)</li> <li>RAM: 512GB</li> <li>Network links<ul> <li>BMC (VHIR)</li> <li>Internal</li> </ul> </li> </ul> </li> </ul>"},{"location":"over/","title":"Overview","text":""},{"location":"over/#particular-considerations","title":"Particular considerations","text":"<p>The cluster follows a typical basic layout. But since it's a simple cluster, the following properties apply:</p> <ul> <li>Login, cluster management and task submission roles are concentrated in one node.</li> <li>There is no dedicated (fast, low-latency) storage network. Service, provision, management and application data transfer are carried over the same regular internal ethernet network. </li> </ul>"},{"location":"over/#diagram","title":"Diagram","text":"<p>To BE DONE: include specific diagram.</p> <p></p>"},{"location":"softusers/","title":"System-wide installed software","text":""},{"location":"softusers/#binaries-libraries-and-runtimes","title":"Binaries, libraries and runtimes","text":"<p>Programs and associated system-wide elements that users can run on the cluster as computing tasks are installed on shared locations and configured to be accessible by SLURM. </p> <p>Most of these software elements have been built from sources, either using regular procedures or relying on automated building and dependencies management systems (EasyBuild and Spack). </p> <p>Recent common compilers and development libraries are available (gnu12, OpenMPI4, MPICH).</p> <p>ALso, recent versions of Python, R, Perl, Java and other runtimes and development tools are avalible. However, users that require a high level of version and optional packages for some development environments (usually with R, Python and others) specificity, are encouraged to deploy and use their own tools and packages at user level (using some conda flavour or Singularity containers)</p>"},{"location":"softusers/#environment-modules","title":"Environment Modules","text":"<p>The lmod implementation of Environment Modules is available. Users are instructed and strongly advised to use this system for submitting computing tasks on the cluster.</p>"}]}